{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NnOoFtyHilj"
      },
      "source": [
        "# Human Activity Recognition Using WiFi Signals\n",
        "\n",
        "## Overview\n",
        "Human Activity Recognition (HAR) using WiFi signals leverages the unique properties of wireless channel variations to detect different activities.\n",
        "\n",
        "## Data Format\n",
        "- **WiFi signal data** is similar to image data in structure, represented in the shape `(channels, height, width)`, but with a different interpretation:\n",
        "  - `channels` → **channel**\n",
        "  - `height` → **Time Steps**\n",
        "  - `width` → **Antenna Pairs (transmitter-receiver combinations)**\n",
        "- **Labels** represent a predefined set of classes, as is typical in classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GqPqJNJIWNg"
      },
      "source": [
        "# Reading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "3DwIkG1iHVPg",
        "outputId": "62d1fe1d-3044-4657-8d69-ef2a7d352b4c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=17Vfiu90uYeeRqmW-QbhocgBt69mrrScA\n",
            "From (redirected): https://drive.google.com/uc?id=17Vfiu90uYeeRqmW-QbhocgBt69mrrScA&confirm=t&uuid=f748bc70-bb81-4692-9dbe-797a495b84f6\n",
            "To: c:\\KAUST Academy\\Stage-2\\coding_exam\\WiFiSensingDataset.pt.zip\n",
            "100%|██████████| 214M/214M [01:02<00:00, 3.41MB/s] \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'WiFiSensingDataset.pt.zip'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gdown\n",
        "\n",
        "link = \"https://drive.google.com/file/d/17Vfiu90uYeeRqmW-QbhocgBt69mrrScA/view?usp=sharing\"\n",
        "file_id = link.split('/d/')[1].split('/')[0]\n",
        "\n",
        "download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "\n",
        "output_file = \"WiFiSensingDataset.pt.zip\"\n",
        "gdown.download(download_url, output_file, quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zg4A9hDI48T",
        "outputId": "6bb440f1-63cb-4737-822c-6782b53cc3f1"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = r'c:\\KAUST Academy\\Stage-2\\coding_exam\\WiFiSensingDataset.pt.zip'\n",
        "extract_to = r'c:\\KAUST Academy\\Stage-2\\coding_exam'\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\afiya\\AppData\\Local\\Temp\\ipykernel_1340\\3046838490.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  data = torch.load(r'c:\\KAUST Academy\\Stage-2\\coding_exam\\WiFiSensingDataset.pt')\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "data = torch.load(r'c:\\KAUST Academy\\Stage-2\\coding_exam\\WiFiSensingDataset.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gKv4rUCekmwu"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'X_test': tensor([[[[0.4063, 0.1338, 0.4588,  ..., 0.6775, 0.7083, 0.6615],\n",
              "           [0.3715, 0.2196, 0.4792,  ..., 0.6935, 0.6967, 0.6641],\n",
              "           [0.3742, 0.2473, 0.4946,  ..., 0.7042, 0.7097, 0.6790],\n",
              "           ...,\n",
              "           [0.5245, 0.4429, 0.3430,  ..., 0.7161, 0.7224, 0.6850],\n",
              "           [0.4956, 0.4228, 0.3719,  ..., 0.7076, 0.6951, 0.6784],\n",
              "           [0.4861, 0.3396, 0.3864,  ..., 0.7166, 0.7392, 0.6788]]],\n",
              " \n",
              " \n",
              "         [[[0.5485, 0.5999, 0.6332,  ..., 0.7803, 0.7650, 0.7257],\n",
              "           [0.5426, 0.5947, 0.6169,  ..., 0.7856, 0.7601, 0.7224],\n",
              "           [0.5658, 0.6009, 0.6354,  ..., 0.7856, 0.7708, 0.7141],\n",
              "           ...,\n",
              "           [0.5661, 0.6048, 0.6422,  ..., 0.7963, 0.7836, 0.7276],\n",
              "           [0.5395, 0.6129, 0.6306,  ..., 0.7933, 0.7771, 0.7327],\n",
              "           [0.5523, 0.5882, 0.6299,  ..., 0.7900, 0.7739, 0.7293]]],\n",
              " \n",
              " \n",
              "         [[[0.3518, 0.4836, 0.4677,  ..., 0.8025, 0.7917, 0.7716],\n",
              "           [0.4130, 0.4547, 0.4947,  ..., 0.8142, 0.7906, 0.7653],\n",
              "           [0.4119, 0.4632, 0.5011,  ..., 0.7962, 0.7977, 0.7453],\n",
              "           ...,\n",
              "           [0.4476, 0.4848, 0.4603,  ..., 0.8009, 0.7937, 0.7510],\n",
              "           [0.4313, 0.4796, 0.5078,  ..., 0.8021, 0.7790, 0.7577],\n",
              "           [0.4664, 0.4507, 0.4557,  ..., 0.7999, 0.7948, 0.7412]]],\n",
              " \n",
              " \n",
              "         ...,\n",
              " \n",
              " \n",
              "         [[[0.3576, 0.3716, 0.4629,  ..., 0.7683, 0.7666, 0.7621],\n",
              "           [0.2200, 0.2692, 0.4988,  ..., 0.7696, 0.7813, 0.7444],\n",
              "           [0.1683, 0.3531, 0.5505,  ..., 0.7591, 0.7936, 0.7436],\n",
              "           ...,\n",
              "           [0.6513, 0.6223, 0.6524,  ..., 0.6487, 0.6314, 0.6023],\n",
              "           [0.6346, 0.6073, 0.6269,  ..., 0.6551, 0.6167, 0.5842],\n",
              "           [0.6494, 0.5845, 0.6663,  ..., 0.6764, 0.6471, 0.5838]]],\n",
              " \n",
              " \n",
              "         [[[0.3932, 0.5839, 0.5014,  ..., 0.9296, 0.7705, 0.7355],\n",
              "           [0.3892, 0.5936, 0.4899,  ..., 0.9323, 0.7967, 0.7201],\n",
              "           [0.3817, 0.5714, 0.4761,  ..., 0.9343, 0.7767, 0.7218],\n",
              "           ...,\n",
              "           [0.2492, 0.4894, 0.3989,  ..., 0.8397, 0.6663, 0.6404],\n",
              "           [0.2035, 0.4521, 0.2779,  ..., 0.8419, 0.6882, 0.6354],\n",
              "           [0.1877, 0.4128, 0.3429,  ..., 0.8454, 0.6681, 0.6456]]],\n",
              " \n",
              " \n",
              "         [[[0.5169, 0.8046, 0.5107,  ..., 0.9392, 0.6567, 0.6159],\n",
              "           [0.5091, 0.8025, 0.5426,  ..., 0.9478, 0.6677, 0.5972],\n",
              "           [0.5129, 0.7894, 0.5407,  ..., 0.9379, 0.6472, 0.5938],\n",
              "           ...,\n",
              "           [0.4086, 0.7455, 0.5057,  ..., 0.9397, 0.6690, 0.6103],\n",
              "           [0.4655, 0.7614, 0.4939,  ..., 0.9387, 0.6515, 0.5977],\n",
              "           [0.4785, 0.7844, 0.4999,  ..., 0.9260, 0.6573, 0.5807]]]]),\n",
              " 'X_train': tensor([[[[0.5931, 0.6234, 0.6664,  ..., 0.8558, 0.8676, 0.8395],\n",
              "           [0.5971, 0.6378, 0.6707,  ..., 0.8611, 0.8697, 0.8310],\n",
              "           [0.6177, 0.6463, 0.7014,  ..., 0.8671, 0.8882, 0.8355],\n",
              "           ...,\n",
              "           [0.5660, 0.6178, 0.6437,  ..., 0.8034, 0.8093, 0.7637],\n",
              "           [0.5688, 0.5915, 0.6619,  ..., 0.8008, 0.8138, 0.7581],\n",
              "           [0.5595, 0.6065, 0.6748,  ..., 0.8052, 0.8233, 0.7676]]],\n",
              " \n",
              " \n",
              "         [[[0.5220, 0.5388, 0.5787,  ..., 0.7684, 0.7890, 0.7382],\n",
              "           [0.5211, 0.5279, 0.5596,  ..., 0.7596, 0.7905, 0.7388],\n",
              "           [0.5103, 0.5240, 0.5150,  ..., 0.7715, 0.7673, 0.7531],\n",
              "           ...,\n",
              "           [0.5267, 0.5472, 0.5706,  ..., 0.7756, 0.7785, 0.7378],\n",
              "           [0.5285, 0.5403, 0.5952,  ..., 0.7585, 0.7750, 0.7280],\n",
              "           [0.5204, 0.5663, 0.5723,  ..., 0.7658, 0.7716, 0.7447]]],\n",
              " \n",
              " \n",
              "         [[[0.5269, 0.3803, 0.5683,  ..., 0.7597, 0.7634, 0.7370],\n",
              "           [0.5132, 0.4223, 0.5921,  ..., 0.7803, 0.7726, 0.7488],\n",
              "           [0.5194, 0.3917, 0.5776,  ..., 0.7802, 0.7822, 0.7389],\n",
              "           ...,\n",
              "           [0.6301, 0.6589, 0.7244,  ..., 0.7481, 0.7587, 0.7165],\n",
              "           [0.6784, 0.6870, 0.7562,  ..., 0.7673, 0.7829, 0.7350],\n",
              "           [0.6772, 0.7130, 0.7709,  ..., 0.7888, 0.7750, 0.7567]]],\n",
              " \n",
              " \n",
              "         ...,\n",
              " \n",
              " \n",
              "         [[[0.4552, 0.4701, 0.4518,  ..., 0.7627, 0.7260, 0.7017],\n",
              "           [0.4510, 0.4539, 0.4680,  ..., 0.7658, 0.7467, 0.6873],\n",
              "           [0.4726, 0.4791, 0.4577,  ..., 0.7637, 0.7276, 0.6952],\n",
              "           ...,\n",
              "           [0.6184, 0.6350, 0.6435,  ..., 0.7921, 0.7995, 0.7435],\n",
              "           [0.6314, 0.6495, 0.6705,  ..., 0.7955, 0.8127, 0.7490],\n",
              "           [0.6347, 0.6501, 0.6485,  ..., 0.8240, 0.8123, 0.7713]]],\n",
              " \n",
              " \n",
              "         [[[0.4525, 0.5108, 0.4933,  ..., 0.8108, 0.7894, 0.7809],\n",
              "           [0.4439, 0.5042, 0.5226,  ..., 0.8034, 0.8099, 0.7532],\n",
              "           [0.4861, 0.5073, 0.4874,  ..., 0.8033, 0.7922, 0.7790],\n",
              "           ...,\n",
              "           [0.5681, 0.5712, 0.5650,  ..., 0.8260, 0.8156, 0.7914],\n",
              "           [0.5283, 0.5525, 0.5790,  ..., 0.8241, 0.8163, 0.7958],\n",
              "           [0.5592, 0.6028, 0.5930,  ..., 0.8242, 0.8168, 0.7940]]],\n",
              " \n",
              " \n",
              "         [[[0.4707, 0.4942, 0.5336,  ..., 0.7695, 0.7696, 0.7393],\n",
              "           [0.4660, 0.5158, 0.5476,  ..., 0.7733, 0.7720, 0.7415],\n",
              "           [0.4900, 0.4933, 0.5147,  ..., 0.7606, 0.7728, 0.7338],\n",
              "           ...,\n",
              "           [0.4940, 0.5088, 0.5460,  ..., 0.7620, 0.7791, 0.7266],\n",
              "           [0.4624, 0.5207, 0.5479,  ..., 0.7745, 0.7776, 0.7458],\n",
              "           [0.4936, 0.5078, 0.5261,  ..., 0.7644, 0.7582, 0.7374]]]]),\n",
              " 'y_test': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
              "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "         1., 1., 1., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
              "         3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
              "         3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 4.,\n",
              "         4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "         4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "         4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "         4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "         4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "         4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
              "         4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 5., 5., 5., 5., 5., 5.,\n",
              "         5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5.,\n",
              "         5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 6., 6.,\n",
              "         6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6.,\n",
              "         6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 2., 2., 2., 2., 2., 2., 2.,\n",
              "         2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
              "         2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
              "         2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
              "         2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
              "         2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
              "         2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
              "         2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
              "         2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]),\n",
              " 'y_train': tensor([2., 1., 2.,  ..., 0., 4., 1.])}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vkoyStvJBLl"
      },
      "source": [
        "# Task 1: Analyze the Dataset ( Stored in `data`)\n",
        "\n",
        "1. **Determine the number of unique labels** in the dataset.  \n",
        "\n",
        "2. **Determine the shape of the input data** (number of samples and features).  \n",
        "\n",
        "3. **Find the maximum value** in the dataset.  \n",
        "\n",
        "4. **Find the minimum value** in the dataset.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract data from the dictionary\n",
        "X_train = data['X_train']  # Training signals tensor\n",
        "X_test  = data['X_test']   # Testing signals tensor\n",
        "y_train = data['y_train']  # Training labels tensor\n",
        "y_test  = data['y_test']   # Testing labels tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert tensors to NumPy arrays\n",
        "X_train_np = X_train.cpu().numpy() if isinstance(X_train, torch.Tensor) else np.array(X_train)\n",
        "X_test_np  = X_test.cpu().numpy()  if isinstance(X_test, torch.Tensor)  else np.array(X_test)\n",
        "y_train_np = y_train.cpu().numpy() if isinstance(y_train, torch.Tensor) else np.array(y_train)\n",
        "y_test_np  = y_test.cpu().numpy()  if isinstance(y_test, torch.Tensor)  else np.array(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique labels: [0. 1. 2. 3. 4. 5. 6.]\n",
            "Number of unique labels: 7\n"
          ]
        }
      ],
      "source": [
        "# Determine the number of unique labels\n",
        "# Concatenate training and testing labels to consider all samples:\n",
        "all_labels = np.concatenate([y_train_np, y_test_np])\n",
        "unique_labels = np.unique(all_labels)\n",
        "print(\"Unique labels:\", unique_labels)\n",
        "print(\"Number of unique labels:\", len(unique_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape: (2500, 1, 250, 90)\n",
            "X_test shape: (500, 1, 250, 90)\n"
          ]
        }
      ],
      "source": [
        "# Determine the shape of the input data\n",
        "print(\"X_train shape:\", X_train_np.shape)\n",
        "print(\"X_test shape:\", X_test_np.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training samples: 2500\n",
            "Number of testing samples: 500\n"
          ]
        }
      ],
      "source": [
        "num_samples_train = X_train_np.shape[0]\n",
        "num_samples_test  = X_test_np.shape[0]\n",
        "\n",
        "print(\"Number of training samples:\", num_samples_train)\n",
        "print(\"Number of testing samples:\", num_samples_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature shape per sample (from training data): (1, 250, 90)\n"
          ]
        }
      ],
      "source": [
        "# print the feature shape per sample (channels, time_steps, antenna_pairs)\n",
        "print(\"Feature shape per sample (from training data):\", X_train_np.shape[1:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Maximum value in the dataset: 1.0\n"
          ]
        }
      ],
      "source": [
        "# Find the maximum value in the dataset (input signals)\n",
        "max_val_train = np.max(X_train_np)\n",
        "max_val_test  = np.max(X_test_np)\n",
        "max_val = max(max_val_train, max_val_test)\n",
        "print(\"Maximum value in the dataset:\", max_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Minimum value in the dataset: 0.0\n"
          ]
        }
      ],
      "source": [
        "# Find the minimum value in the dataset\n",
        "min_val_train = np.min(X_train_np)\n",
        "min_val_test  = np.min(X_test_np)\n",
        "min_val = min(min_val_train, min_val_test)\n",
        "print(\"Minimum value in the dataset:\", min_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbbW9TqJJI84"
      },
      "source": [
        "# Task 2: Build and Evaluate a Neural Network\n",
        "\n",
        "1. **Design a Neural Network (Maximum 5 Layers)**  \n",
        "   Build a compact neural network with no more than 5 layers. Clearly specify the type of each layer (e.g., Dense, Conv2D) and any activation functions used.\n",
        "\n",
        "2. **Evaluate Your Model**  \n",
        "   Train your network on the provided dataset and report the evaluation metrics (e.g., accuracy, loss). Discuss the performance of your model and any challenges faced during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "5er_pdi8w5cV"
      },
      "outputs": [],
      "source": [
        "\n",
        "class WiFiHARNet(nn.Module):\n",
        "    def __init__(self, num_classes=7):\n",
        "        super(WiFiHARNet, self).__init__()\n",
        "        # Layer 1: Convolutional layer (Conv2D)\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
        "        # Layer 2: Convolutional layer (Conv2D)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
        "        # Pooling layer (not counted as learnable)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        # Compute the flattened feature size.\n",
        "        # Input shape: (1, 250, 90) → after conv1: (16, 250, 90) → after conv2: (32, 250, 90)\n",
        "        # After pooling, spatial dims become (250/2, 90/2) = (125, 45)\n",
        "        flattened_size = 32 * 125 * 45\n",
        "        \n",
        "        # Layer 3: Fully Connected (Dense) layer\n",
        "        self.fc1 = nn.Linear(flattened_size, 64)\n",
        "        # Layer 4: Output layer (Fully Connected)\n",
        "        self.fc2 = nn.Linear(64, num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, 1, 250, 90)\n",
        "        x = F.relu(self.conv1(x))   # -> (batch_size, 16, 250, 90)\n",
        "        x = F.relu(self.conv2(x))   # -> (batch_size, 32, 250, 90)\n",
        "        x = self.pool(x)            # -> (batch_size, 32, 125, 45)\n",
        "        x = x.view(x.size(0), -1)   # flatten to (batch_size, 32*125*45)\n",
        "        x = F.relu(self.fc1(x))     # -> (batch_size, 64)\n",
        "        x = self.fc2(x)             # -> (batch_size, 7)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WiFiHARNet(\n",
            "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=180000, out_features=64, bias=True)\n",
            "  (fc2): Linear(in_features=64, out_features=7, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Instantiate and print the model architecture\n",
        "model = WiFiHARNet(num_classes=7)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 - Loss: 1.9828, Accuracy: 31.80%\n",
            "Epoch 2/10 - Loss: 1.3575, Accuracy: 49.60%\n",
            "Epoch 3/10 - Loss: 1.0862, Accuracy: 59.52%\n",
            "Epoch 4/10 - Loss: 0.8621, Accuracy: 68.72%\n",
            "Epoch 5/10 - Loss: 0.7060, Accuracy: 75.68%\n",
            "Epoch 6/10 - Loss: 0.6133, Accuracy: 79.12%\n",
            "Epoch 7/10 - Loss: 0.4882, Accuracy: 84.28%\n",
            "Epoch 8/10 - Loss: 0.3700, Accuracy: 87.60%\n",
            "Epoch 9/10 - Loss: 0.3502, Accuracy: 88.76%\n",
            "Epoch 10/10 - Loss: 0.2893, Accuracy: 90.44%\n"
          ]
        }
      ],
      "source": [
        "# Convert data types if necessary\n",
        "X_train = X_train.float()  # shape: (2500, 1, 250, 90)\n",
        "X_test  = X_test.float()   # shape: (500, 1, 250, 90)\n",
        "y_train = y_train.long()   # labels as integers\n",
        "y_test  = y_test.long()\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "test_dataset  = TensorDataset(X_test, y_test)\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10  # Adjust as needed\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, targets in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = correct / total * 100\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 0.5807, Test Accuracy: 80.20%\n"
          ]
        }
      ],
      "source": [
        "# Evaluation on the test set\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        test_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "\n",
        "test_loss = test_loss / total\n",
        "test_acc = correct / total * 100\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
